{
  "hash": "20dbe64093cc8328a204a60530555644",
  "result": {
    "markdown": "---\ntitle: \"Week 6\"\ndescription: \"Common families of discrete distributions cont\"\ndate: \"10/30/2023\"\ndate-modified: \"10/24/2023\"\ncategories: [\"\"]\nformat: \n  html:\n    link-external-newwindow: true\n    toc: true\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n<style type=\"text/css\">\n.title{\n  font-size: 40px;\n  color: #006a4e;\n  background-color: #fff;\n  padding: 10px;\n}\n\n.description{\n  font-size: 20px;\n  color: #fff;\n  background-color: #006a4e;\n  padding: 10px;\n}\n</style>\n:::\n\n\n## Resources\n\n| Chapter | Topic                |                                Slides                                 |                                  Annotated Slides                                  |                           Recording                           |\n|-------------|-------------|:-------------:|:-----------------:|:-----------:|\n| 17      | Negative Binomial RV | [{{< iconify ri slideshow-fill size=30px >}}]{style=\"color:#f8f5f0;\"} | [{{< iconify pepicons-pop pen-circle-filled size=29px >}}]{style=\"color:#f8f5f0;\"} | [{{< iconify mdi video size=34px >}}]{style=\"color:#f8f5f0;\"} |\n| 18      | Poisson RV           | [{{< iconify ri slideshow-fill size=30px >}}]{style=\"color:#f8f5f0;\"} | [{{< iconify pepicons-pop pen-circle-filled size=29px >}}]{style=\"color:#f8f5f0;\"} | [{{< iconify mdi video size=34px >}}]{style=\"color:#f8f5f0;\"} |\n\n### Exam Materials\n\n-   The exam will cover all materials from Week 1 - Week 4\n\n    -   This translates to Chapter 1-11 and 22\n\n    -   This also translates to HW 1-4\n\n-   Practice materials\n\n    -   You can go back to do the Extra Problems from each homework\n\n    -   [Exam Review](/weeks/week_06/midterm_prep_materials/Exam_1_review_F16-F20.pdf) document (made by Meike in 2021)\n\n        -   [Exam Review Answers](/weeks/week_06/midterm_prep_materials/Exam_1_review_F16-F20_answers.pdf) (made by Meike in 2021)\n\n        -   Questions that are **not** covered on our exam:\n\n            -   Fall 2020: 5\n\n            -   Fall 2019: 5b\n\n            -   Fall 2018: 5b\n\n            -   Fall 2017: 5c, 6\n\n            -   Fall 2016: 5b, 6\n\n        -   Also, don't worry too much about the problems with chairs in a circle - we never covered these in detail in class\n\n            -   If you're curious, the book does have a few trick for these problems\n\n## Class Exit Tickets\n\n[[{{< iconify healthicons health-worker-form-negative size=30px >}}](https://forms.office.com/Pages/ResponsePage.aspx?id=V3lz4rj6fk2U9pvWr59xWFMopmPUjRtDiHLswhgacNhUQTlMT0RUUUJDUTVTRVBUQjlYNlMzS0FVRC4u)]{style=\"color:#f8f5f0;\"} Monday (10/30)\n\n## Additional Information\n\n### Announcements 10/30\n\n-   Any questions about the exam on Wednesday?\n\n-   On the Monday and Tuesday before the exam, I will hold virtual office hours\n\n    -   Monday, 10/30, 4-5pm\n\n    -   Tuesday, 10/31, 1:30-2:30pm\n\n    -   If you cannot make those hours and want to ask me questions, we can set up a short meeting for Monday or Tuesday\n\n## Statistician of the Week: Lester Mackey\n\n::: grid\n::: g-col-4\n\n::: {.cell preview='true'}\n::: {.cell-output-display}\n![Lester Mackey](../images/stat_of_week/mackey.jpeg){fig-alt='Image credit: Dana J Quigley'}\n:::\n:::\n\n:::\n\n::: g-col-8\nDr. Mackey is a machine learning researcher at Microsoft Research New England and an adjunct professor at Stanford University. His PhD (Computer Science 2012) and MA (Statistics 2011) are both from University of California, Berkeley, while his undergraduate degree (Computer Science 2007) is from Princeton University.\n\nHe is involved in Stanford's initiative of <a href = \"https://stats-for-good.stanford.edu/\" target = \"_blank\">Statistics for Social Good</a> and has the following quote on his website:\n\n> Quixotic though it may sound, I hope to use computer science and statistics to change the world for the better.\n:::\n:::\n\n#### Topics covered\n\nFrom Dr. Mackey's <a href = \"http://stanford.edu/~lmackey/\" target = \"_blank\">personal website</a> his areas of research are:\n\n-   statistical machine learning\n-   scalable algorithms\n-   high-dimensional statistics\n-   approximate inference\n-   probability\n\n#### Relevant work\n\n-   Koulik Khamaru, Yash Deshpande, Lester Mackey, and Martin J. Wainwright, <a href = \"https://arxiv.org/pdf/2107.02266.pdf\" target = \"_blank\">Near-optimal inference in adaptive linear regression</a>\n\n> When data is collected in an adaptive manner, even simple methods like ordinary least squares can exhibit non-normal asymptotic behavior. As an undesirable consequence, hypothesis tests and confidence intervals based on asymptotic normality can lead to erroneous results. We propose a family of online debiasing estimators to correct these distributional anomalies in least squares estimation. Our proposed methods take advantage of the covariance structure present in the dataset and provide sharper estimates in directions for which more information has accrued. We establish an asymptotic normality property for our proposed online debiasing estimators under mild conditions on the data collection process and provide asymptotically exact confidence intervals...\n\n-   Pierre Bayle, Alexandre Bayle, Lucas Janson, and Lester Mackey, <a href = \"https://proceedings.neurips.cc/paper/2020/hash/bce9abf229ffd7e570818476ee5d7dde-Abstract.html\" target = \"_blank\">Cross-validation Confidence Intervals for Test Error</a> **Advances in Neural Information Processing Systems (NeurIPS)**, December 2020.\n\n> This work develops central limit theorems for cross-validation and consistent estimators of its asymptotic variance under weak stability conditions on the learning algorithm. Together, these results provide practical, asymptotically-exact confidence intervals for k-fold test error and valid, powerful hypothesis tests of whether one learning algorithm has smaller k-fold test error than another. These results are also the first of their kind for the popular choice of leave-one-out cross-validation. In our real-data experiments with diverse learning algorithms, the resulting intervals and tests outperform the most popular alternative methods from the literature...\n\n#### Outside links\n\n-   [Mathematically Gifted & Black](https://mathematicallygiftedandblack.com/honorees/lester-mackey/)\n-   [Linkedin](https://www.linkedin.com/in/lester-mackey-5902909/)\n-   [personal](http://stanford.edu/~lmackey/)\n\n**Please note the statisticians of the week are taken directly from the [CURV project by Jo Hardin](https://hardin47.github.io/CURV/).**\n\n#### Other\n\nThe precursor to <a href = \"https://www.kaggle.com/\" target = \"_blank\">kaggle</a> was a <a href = \"https://en.wikipedia.org/wiki/Netflix_Prize\" target = \"_blank\">\\$1 million prize given by Netflix</a> to the most accurate prediction of ratings that people give to the movies they watch. As undergraduates, Dr. Mackey and two friends led the competition for a few hours in its first year. Later, groups merged and Dr. Mackey's group merged with a few others, forming The Ensemble. Their final analysis came in second with the **exact same** error rates as the winning entry. The winning entry, however, had been submitted 20 minutes prior. Sigh.\n\n## Muddiest Points\n\nThis will be filled in with your Exit Ticket responses.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}